---
title: "Week 8- Homework Solutions"
author: "Mohit Khanna"
date: "28/06/2019"
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---

***

```{r setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.alin = "center")
```

## Exercise 1 (Writing Functions)

**(a)** Write a function named `diagnostics` that takes as input the arguments:

- `model`, an object of class `lm()`, that is a model fit via `lm()`
- `pcol`, for controlling point colors in plots, with a default value of `grey`
- `lcol`, for controlling line colors in plots, with a default value of `dodgerblue`
- `alpha`, the significance level of any test that will be performed inside the function, with a default value of `0.05`
- `plotit`, a logical value for controlling display of plots with default value `TRUE`
- `testit`, a logical value for controlling outputting the results of tests with default value `TRUE`

The function should output:

- A list with two elements when `testit` is `TRUE`:
    - `p_val`, the p-value for the Shapiro-Wilk test for assessing normality
    - `decision`, the decision made when performing the Shapiro-Wilk test using the `alpha` value input to the function. "Reject" if the null hypothesis is rejected, otherwise "Fail to Reject."
- Two plots, side-by-side, when `plotit` is `TRUE`:
    - A fitted versus residuals plot that adds a horizontal line at $y = 0$, and labels the $x$-axis "Fitted" and the $y$-axis "Residuals." The points and line should be colored according to the input arguments. Give the plot a title. 
    - A Normal Q-Q plot of the residuals that adds the appropriate line using `qqline()`. The points and line should be colored according to the input arguments. Be sure the plot has a title. 

Consider using this function to help with the remainder of the assignment as well.

***Solution:***
<br>

```{r}
diagnostics <-function(model, pcol="grey", lcol="dodgerblue", alpha=0.05, plotit=TRUE,testit=TRUE){
  if (plotit==TRUE){
    par(mfrow=c(1,2))
    plot(fitted(model), resid(model), col = pcol, pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residual")
    abline(h = 0, col = lcol, lwd = 2)
    
    qqnorm(resid(model), main = "Q-Q Plot", col = pcol)
    qqline(resid(model), col = lcol, lwd = 2)
  }
  if (testit==TRUE){
  p_val=shapiro.test(resid(model))[["p.value"]]
  decision=ifelse(p_val<alpha, "Reject", "Fail to Reject")
  return (list("p_val"=p_val, "decision"=decision))
  }
}
```

<br>


**(b)** Run the following code.

```{r}
set.seed(420)

data_1 = data.frame(x = runif(n = 30, min = 0, max = 10),
                    y = rep(x = 0, times = 30))
data_1$y = with(data_1, 2 + 1 * x + rexp(n = 30))
fit_1 = lm(y ~ x, data = data_1)

data_2 = data.frame(x = runif(n = 20, min = 0, max = 10),
                    y = rep(x = 0, times = 20))
data_2$y = with(data_2, 5 + 2 * x + rnorm(n = 20))
fit_2 = lm(y ~ x, data = data_2)

data_3 = data.frame(x = runif(n = 40, min = 0, max = 10),
                    y = rep(x = 0, times = 40))
data_3$y = with(data_3, 2 + 1 * x + rnorm(n = 40, sd = x))
fit_3 = lm(y ~ x, data = data_3)
```

```{r}
diagnostics(fit_1, plotit = FALSE)$p_val
diagnostics(fit_2, plotit = FALSE)$decision
diagnostics(fit_1, testit = FALSE, pcol = "black", lcol = "black")
diagnostics(fit_2, testit = FALSE, pcol = "grey", lcol = "green")
diagnostics(fit_3)
```

***

## Exercise 2 (Prostate Cancer Data)

For this exercise, we will use the `prostate` data, which can be found in the `faraway` package. After loading the `faraway` package, use `?prostate` to learn about this dataset.

```{r, message = FALSE, warning = FALSE}
library(faraway)
library(lmtest)
```

**(a)** Fit an additive multiple regression model with `lpsa` as the response and the remaining variables in the `prostate` dataset as predictors. Report the $R^2$ value for this model.

***Solution:***

<br>

```{r}
pros_mod=lm(lpsa~., data=prostate)
summary(pros_mod)[["r.squared"]]
```

<br>

**(b)** Check the constant variance assumption for this model. Do you feel it has been violated? Justify your answer.



***Solution:***

<br>

```{r}
bptest(pros_mod)
plot(fitted(pros_mod), resid(pros_mod), col = "dodgerblue", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residual")
abline(h = 0, col = "orange", lwd = 2)
```

<br>

Doing the BP test shows less p-value > 0.05, the fitted vs residual plot shows no signs of different variance, thus the model does not violate the equal variance assumption.

<br>

**(c)** Check the normality assumption for this model. Do you feel it has been violated? Justify your answer.



***Solution:***

<br>

```{r}
shapiro.test(resid(pros_mod)) 
qqnorm(resid(pros_mod), main = "Q-Q Plot", col = "dodgerblue")
qqline(resid(pros_mod), col = "dodgerblue", lwd = 2)
```

<br>

Doing the Shapiro test shows less p-value > 0.05, the q-q plot looks normal, thus the model does not violate the normality assumption.

<br>

**(d)** Check for any high leverage observations. Report any observations you determine to have high leverage.

***Solution:***

<br>

```{r}
high_hat_obs=hatvalues(pros_mod) > 2 * mean(hatvalues(pros_mod))
subset(prostate, high_hat_obs)
```

<br>

The following observations have high leverage measured as one having leverage greater than 2 times the mean of all leverages.

<br>

**(e)** Check for any influential observations. Report any observations you determine to be influential.

***Solution:***

<br>

```{r}
cook_dist=cooks.distance(pros_mod)
subset(prostate, cook_dist > 4 / length(cook_dist))
```

<br>

The following observations are influential observations measured as one having cooks distance greater than 4/n .


<br>

**(f)** Refit the additive multiple regression model without any points you identified as influential. Compare the coefficients of this fitted model to the previously fitted model.

***Solution:***

<br>

```{r}

pros_mod_2 = lm(lpsa~., data=prostate,
                    subset = cook_dist <= 4 / length(cook_dist))

unname(coef(pros_mod))
unname(coef(pros_mod_2))
```


Here we can see that once we remove the influential points we get different parameter estimates and one noticeable difference is that the intercept goes from positive to negative and thus it looks like the influential points had much impact on the model since after removing them we see a lot of changes in parameters which is something which we will not observe incase the points are normal.

<br>

**(g)** Create a data frame that stores the observations that were "removed" because they were influential. Use the two models you have fit to make predictions with these observations. Comment on the difference between these two sets of predictions.

***Solution:***

<br>

```{r}
infludencial_points =data.frame(subset(prostate, cook_dist > 4 / length(cook_dist)))
predict(pros_mod, newdata = infludencial_points)
predict(pros_mod_2, newdata = infludencial_points)
```

So we can see that when we make predictions for the influential points from the 2 models we get different results which is obvious since we saw above that the 2 models had different parameter estimates. For some observations the predictions are quite different indicating of potential extrapolations that could be happening.


<br>

***

## Exercise 3 (Why Bother?)

**Why** do we care about violations of assumptions? One key reason is that the distributions of the parameter estimators that we have used are all reliant on these assumptions. When the assumptions are violated, the distributional results are not correct, so our tests are garbage. **Garbage In, Garbage Out!**

Consider the following setup that we will use for the remainder of the exercise. We choose a sample size of 50.

```{r}
n = 50
set.seed(420)
x_1 = runif(n, 0, 5)
x_2 = runif(n, -2, 2)
```

Consider the model,

\[
Y = 4 + 1 x_1 + 0 x_2 + \epsilon.
\]

That is,

- $\beta_0$ = 4
- $\beta_1$ = 1
- $\beta_2$ = 0

We now simulate `y_1` in a manner that does **not** violate any assumptions, which we will verify. In this case $\epsilon \sim N(0, 1).$

```{r}
set.seed(1)
library(lmtest)
y_1 = 4 + 1 * x_1 + 0 * x_2 + rnorm(n = n, mean = 0, sd = 1)
fit_1 = lm(y_1 ~ x_1 + x_2)
bptest(fit_1)
```

Then, we simulate `y_2` in a manner that **does** violate assumptions, which we again verify. In this case $\epsilon \sim N(0, \sigma = |x_2|).$

```{r}
set.seed(1)
y_2 = 4 + 1 * x_1 + 0 * x_2 + rnorm(n = n, mean = 0, sd = abs(x_2))
fit_2 = lm(y_2 ~ x_1 + x_2)
bptest(fit_2)
```

**(a)** Use the following code after changing `birthday` to your birthday.

```{r}
num_sims = 2500
p_val_1 = rep(0, num_sims)
p_val_2 = rep(0, num_sims)
birthday = 19930501
set.seed(birthday)
```

Repeat the above process of generating `y_1` and `y_2` as defined above, and fit models with each as the response `2500` times. Each time, store the p-value for testing,

\[
\beta_2 = 0,
\]

using both models, in the appropriate variables defined above. (You do not need to use a data frame as we have in the past. Although, feel free to modify the code to instead use a data frame.)

***Solution:***

<br>

```{r}
for (i in 1:num_sims){
  y_1 = 4 + 1 * x_1 + 0 * x_2 + rnorm(n = n, mean = 0, sd = 1)
  fit_1 = lm(y_1 ~ x_1 + x_2)
  p_val_1[i]=summary(fit_1)[["coefficients"]][3, "Pr(>|t|)"]
  
  y_2 = 4 + 1 * x_1 + 0 * x_2 + rnorm(n = n, mean = 0, sd = abs(x_2))
  fit_2 = lm(y_2 ~ x_1 + x_2)
  p_val_2[i]=summary(fit_2)[["coefficients"]][3, "Pr(>|t|)"]
}
```

<br>

**(b)** What proportion of the `p_val_1` values is less than 0.01? Less than 0.05? Less than 0.10? What proportion of the `p_val_2` values is less than 0.01? Less than 0.05? Less than 0.10? Arrange your results in a table. Briefly explain these results.

***Solution:***

<br>

```{r}
result_matrix =matrix(0, nrow = 2, ncol = 3)
rownames(result_matrix)=c("p_val_1", "p_val_2")
colnames(result_matrix)=c("p<0.01","p<0.05","p<0.10")
result_matrix[1,]=c(mean(p_val_1<0.01),mean(p_val_1<0.05), mean(p_val_1<0.10))
result_matrix[2,]=c(mean(p_val_2<0.01),mean(p_val_2<0.05), mean(p_val_2<0.10))
knitr::kable(result_matrix)
```

<br>

In case of model 1(fit_1) we would like the test to mostly Fail to reject the null hypothesis for which it needs to get high p value what we see here in the above result is we get pretty small proportion of values that are less than 0.01, 0.05, 0.10 . At 0.10 we get around 9% cases where the test failed.

In case of model 2(fit_2) we would like the test to mostly reject the null hypothesis for which it needs to get low p value what we see here in the above result is we get pretty moderate proportion of values that are less than 0.01, 0.05, 0.10 . At 0.10 we get around 17% cases where the test actually did correct, we would have hoped this to show much larger number.

<br>

***

## Exercise 4 (Corrosion Data)

For this exercise, we will use the `corrosion` data, which can be found in the `faraway` package. After loading the `faraway` package, use `?corrosion` to learn about this dataset.

```{r, message = FALSE, warning = FALSE}
library(faraway)
```

**(a)** Fit a simple linear regression with `loss` as the response and `Fe` as the predictor. Plot a scatterplot and add the fitted line. Check the assumptions of this model.

***Solution:***

<br>

```{r}
corr_mod=lm(loss~Fe, data=corrosion)
par(mfrow=c(1,3))
plot(corrosion$Fe,corrosion$loss, col="dodgerblue",
       ylab="Weight loss in mg",
       xlab="Iron content in percent",
       main="Iron content vs Weight loss",
       pch=20,
       cex=1) 
abline(corr_mod, lwd = 3, col = "darkorange")

plot(fitted(corr_mod), resid(corr_mod), col = "dodgerblue", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residual")
abline(h = 0, col = "orange", lwd = 2)
bptest(corr_mod)


qqnorm(resid(corr_mod), main = "Q-Q Plot", col = "dodgerblue")
qqline(resid(corr_mod), col = "dodgerblue", lwd = 2)
shapiro.test(resid(corr_mod)) 
```

<br>

From the residual plot the values seems to be centered around 0 and have equal variance, the Q-Q plot confirms the normality assumption of the errors and the value of the BP test and Shapiro test confirms the above findings.

<br>

**(b)** Fit higher order polynomial models of degree 2, 3, and 4. For each, plot a fitted versus residuals plot and comment on the constant variance assumption. Based on those plots, which of these three models do you think are acceptable? Use a statistical test(s) to compare the models you just chose. Based on the test, which is preferred? Check the normality assumption of this model. Identify any influential observations of this model.

***Solution:***

<br>

```{r}
par(mfrow=c(1,3))

corr_mod_deg2=lm(loss~Fe+I(Fe^2), data=corrosion)
plot(fitted(corr_mod_deg2), resid(corr_mod_deg2), col = "dodgerblue", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residual for degree 2")
abline(h = 0, col = "orange", lwd = 2)

corr_mod_deg3=lm(loss~Fe+I(Fe^2)+I(Fe^3), data=corrosion)
plot(fitted(corr_mod_deg3), resid(corr_mod_deg3), col = "dodgerblue", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residual for degree 3")
abline(h = 0, col = "orange", lwd = 2)

corr_mod_deg4=lm(loss~Fe+I(Fe^2)+I(Fe^3)+I(Fe^4), data=corrosion)
plot(fitted(corr_mod_deg4), resid(corr_mod_deg4), col = "dodgerblue", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residual for degree 4")
abline(h = 0, col = "orange", lwd = 2)

bptest(corr_mod_deg2)
bptest(corr_mod_deg3)
bptest(corr_mod_deg4)
```

<br>

From the above plots all of them seems to be doing fine in terms of constant variance as eyeballing them does not bring out any noticeable difference.

Since all of them looks similar i will go with the degree 3 model since i see that the values are quite equally distributed around 0 which looks much balanced when compared to other 2 plots. Lets conduct a test and see the results from the test.

```{r}
anova(corr_mod_deg2, corr_mod_deg3, corr_mod_deg4)
```
<br>


Based on the anova test we see that the model 2 which is degree 3 is quite significant as it has very low p value so in this case also we pick up model 2(Degree 3) according to the anova test.

Lets check the normality assumption of the model
```{r}
qqnorm(resid(corr_mod_deg3), main = "Q-Q Plot", col = "dodgerblue")
qqline(resid(corr_mod_deg3), col = "dodgerblue", lwd = 2)
shapiro.test(resid(corr_mod_deg3)) 
```

<br>

Using the Q-Q plot and the Shapiro test we see that the normality assumption is not violated.

Lets check if we have any influential points.

```{r}
cook_dist=cooks.distance(corr_mod_deg3)
subset(corrosion, cook_dist > 4 / length(cook_dist))
```
Looks like we don't have any influential points for this model, which is good.

***

## Exercise 5 (Diamonds)

The data set `diamonds` from the `ggplot2` package contains prices and characteristics of 54,000 diamonds. For this exercise, use `price` as the response variable $y$, and `carat` as the predictor $x$. Use `?diamonds` to learn more.

```{r, message = FALSE, warning = FALSE}
library(ggplot2)
```

**(a)** Fit a linear model with `price` as the response variable $y$, and `carat` as the predictor $x$. Return the summary information of this model.

***Solution:***

<br>

```{r}
dia_mod=lm(price~carat, data=diamonds)
summary(dia_mod)
```

<br>

**(b)** Plot a scatterplot of price versus carat and add the line for the fitted model in part **(a)**. Using a fitted versus residuals plot and/or a Q-Q plot, comment on the diagnostics. 

***Solution:***

<br>

```{r}

plot(diamonds$carat,diamonds$price, col="dodgerblue",
       ylab="price in US dollars",
       xlab="weight of diamond",
       main="Price vs carat",
       pch=20,
       cex=1) 
abline(dia_mod, lwd = 3, col = "darkorange")

par(mfrow=c(1,2))
plot(fitted(dia_mod), resid(dia_mod), col = "dodgerblue", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residual")
abline(h = 0, col = "orange", lwd = 2)

qqnorm(resid(dia_mod), main = "Q-Q Plot", col = "dodgerblue")
qqline(resid(dia_mod), col = "dodgerblue", lwd = 2)
```

<br>

The fitted vs residual plot shows patterns and does not seem to be random, the values are not centered around 0 and the variance is also not equal everywhere. The Q-Q plot also does not look normal , hence the assumptions for the model are heavily violated.


<br>


**(c)** Seeing as the price stretches over several orders of magnitude, it seems reasonable to try a log transformation of the response. Fit a model with a logged response, plot a scatterplot of log-price versus carat and add the line for the fitted model, then use a fitted versus residuals plot and/or a Q-Q plot to comment on the diagnostics of the model.

```{r}
qplot(price, data = diamonds, bins = 30)
```

***Solution:***

<br>

```{r}
dia_mod_log=lm(log(price)~carat, data=diamonds)
plot(diamonds$carat,log(diamonds$price), col="dodgerblue",
       ylab="Log of price in US dollars",
       xlab="weight of diamond",
       main="Price vs carat",
       pch=20,
       cex=1) 
abline(dia_mod_log, lwd = 3, col = "darkorange")

par(mfrow=c(1,2))
plot(fitted(dia_mod_log), resid(dia_mod_log), col = "dodgerblue", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residual")
abline(h = 0, col = "orange", lwd = 2)

qqnorm(resid(dia_mod_log), main = "Q-Q Plot", col = "dodgerblue")
qqline(resid(dia_mod_log), col = "dodgerblue", lwd = 2)

```

We see that after log transformation of response the price values are less skewed but looking at the diagnostic plots it does not look like the model has been helped a lot since we do see a pattern, the values are not centered around 0 and Q-Q plot does show some deviation , but whats worth noting is that for a specific portion of weight values the model does a pretty good job but as the weight increases the model starts violating the assumptions.

<br>

**(d)** Try adding log transformation of the predictor. Fit a model with a logged response and logged predictor, plot a scatterplot of log-price versus log-carat and add the line for the fitted model, then use a fitted versus residuals plot and/or a Q-Q plot to comment on the diagnostics of the model.

***Solution:***

<br>

```{r}

dia_mod_log_pred=lm(log(price)~log(carat), data=diamonds)
plot(log(diamonds$carat),log(diamonds$price), col="dodgerblue",
       ylab="Log of price in US dollars",
       xlab="Log of weight of diamond",
       main="Price vs carat",
       pch=20,
       cex=1) 
abline(dia_mod_log_pred, lwd = 3, col = "darkorange")

par(mfrow=c(1,2))
plot(fitted(dia_mod_log_pred), resid(dia_mod_log_pred), col = "dodgerblue", pch = 20,
     xlab = "Fitted", ylab = "Residuals", main = "Fitted vs Residual")
abline(h = 0, col = "orange", lwd = 2)

qqnorm(resid(dia_mod_log_pred), main = "Q-Q Plot", col = "dodgerblue")
qqline(resid(dia_mod_log_pred), col = "dodgerblue", lwd = 2)

```

<br>

The above transformation improves upon the previous one , now we have a better fitting line and the fitted vs residual plot looks more centered around 0, even though we have some large deviation for larger values of weight, the Q-Q plot also improves and show more normality however that gets violated along the tails.

<br>

**(e)** Use the model from part **(d)** to predict the price (in dollars) of a 3-carat diamond. Construct a 99% prediction interval for the price (in dollars).

***Solution:***

<br>

```{r}
exp(predict(dia_mod_log_pred, newdata = data.frame("carat"=log(3))))

exp(predict(dia_mod_log_pred, newdata = data.frame("carat"=log(3)), level=0.99, interval="prediction"))
```

<br>

***