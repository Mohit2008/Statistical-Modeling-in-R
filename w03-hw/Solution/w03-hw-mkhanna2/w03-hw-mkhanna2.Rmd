---
title: "Week 3 - Homework Solutions"
author: "Mohit Khanna, 671803064, NetId-mkhanna2"
date: '5/31/2019'
output:
  html_document: 
    toc: yes
  pdf_document: default
urlcolor: cyan
---

***


## Exercise 1 (Using `lm` for Inference)

For this exercise we will use the `cats` dataset from the `MASS` package. You should use `?cats` to learn about the background of this dataset.

**(a)** Fit the following simple linear regression model in `R`. Use heart weight as the response and body weight as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Store the results in a variable called `cat_model`. Use a $t$ test to test the significance of the regression. Report the following:

- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.05$
- A conclusion in the context of the problem

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

<br>
***Solution:***
```{r}
library(MASS)
cat_model = lm(Hwt~Bwt, data=cats)
```

- The null and alternative hypotheses

***Null Hypothesis:***
- $H_0: \beta_1 = 0$

There is no significant linear relationship between Heart weight and Body weight. 

***Alternate Hypothesis:***
- $H_1: \beta_1 \neq 0$

There is a significant linear relationship between Heart weight and Body weight. 

***
- The value of the test statistic

The value of test statistic as reported by test for the slope param is:
```{r}
summary(cat_model)$coefficients[2, "t value"]
```

***

- The p-value of the test

The p-value for the slope param is :
```{r}
summary(cat_model)$coefficients[2, "Pr(>|t|)"]
```

***
- A statistical decision at $\alpha = 0.05$

At a alpha level of 0.05 we will reject the null hypothesis since the p-value reported by test statistic is less than the alpha level

***

- A conclusion in the context of the problem

By conducting the t test for the significance of regression we can infer that there is a relationship between the heart weight and body weight and the relationship has a parameter estimate which is statistically significantly from 0 (null hypothesis).

<br>

**(b)** Calculate a 90% confidence interval for $\beta_1$. Give an interpretation of the interval in the context of the problem.

<br>
***Solution:***
```{r}
confint(cat_model,parm="Bwt",level=0.90)
```

We are 90% confident that for an increase in body weight of 1 kg, the true mean increase in the heart weight is between 3.619716 and 4.448409 g.

<br>


**(c)** Calculate a 99% confidence interval for $\beta_0$. Give an interpretation of the interval in the context of the problem.

<br>
***Solution:***
```{r}
confint(cat_model,parm="(Intercept)",level=0.99)
```
We are 99% confident that the true mean heart weight of a cat with 0 body weight is between -2.164125 and 1.4508 g, which might sound a bit odd since if the body weight is 0 how can the heart weight be greater or lesser than that but that's how we interpret the results of the intercept.


<br>


**(d)** Use a 99% confidence interval to estimate the mean heart weight for body weights of 2.1 and 2.8 kilograms. Which of the two intervals is wider? Why?

<br>
***Solution:***
```{r}
predict(cat_model, newdata = data.frame(Bwt=c(2.1, 2.8)), interval = c("confidence"), level = 0.99)
```

The interval for 2.1 is larger, this is due to the fact that since 2.1 is far away from the mean which is 2.723611 and we know that the standard error for the mean prediction depends upon the value of x and its distance from the mean of x , thus a value which is closer to the mean of x will have a smaller range as compared to the one which is farther away.

<br>


**(e)** Use a 99% prediction interval to predict the heart weight for body weights of 2.8 and 4.2 kilograms.

<br>
***Solution:***
```{r}
predict(cat_model, newdata = data.frame(Bwt=c(2.8, 4.2)), interval = c("prediction"), level = 0.99)
```
<br>


**(f)** Create a scatter plot of the data. Add the regression line, 90% confidence bands, and 90% prediction bands.


<br>
***Solution:***
```{r}
body_wt_grid = seq(min(cats$Bwt), max(cats$Bwt), by = 0.01)
dist_ci_band = predict(cat_model, 
                       newdata = data.frame(Bwt = body_wt_grid), 
                       interval = "confidence", level = 0.90)
dist_pi_band = predict(cat_model, 
                       newdata = data.frame(Bwt = body_wt_grid), 
                       interval = "prediction", level = 0.90) 

plot(Hwt ~ Bwt, data = cats,
     xlab = "Body weight of cats in kg",
     ylab = "Heart weight of cats in g",
     main = "Heart weight vs Body weight",
     pch  = 20,
     cex  = 2,
     col  = "grey",
     ylim = c(min(dist_pi_band), max(dist_pi_band)))
abline(cat_model, lwd = 5, col = "darkorange")

lines(body_wt_grid, dist_ci_band[,"lwr"], col = "dodgerblue", lwd = 3, lty = 2)
lines(body_wt_grid, dist_ci_band[,"upr"], col = "dodgerblue", lwd = 3, lty = 2)
lines(body_wt_grid, dist_pi_band[,"lwr"], col = "dodgerblue", lwd = 3, lty = 3)
lines(body_wt_grid, dist_pi_band[,"upr"], col = "dodgerblue", lwd = 3, lty = 3)
points(mean(cats$Bwt), mean(cats$Hwt), pch = "+", cex = 3)
```
<br>

**(g)** Use a $t$ test to test:

- $H_0: \beta_1 = 4$
- $H_1: \beta_1 \neq 4$

Report the following:

- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.05$

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

<br>
***Solution:***

***Null Hypothesis:***
- $H_0: \beta_1 = 4$

The slope parameter for the model is 4. 

***Alternate Hypothesis:***
- $H_1: \beta_1 \neq 4$

The slope param for the model is different from 4. 

- The value of the test statistic
```{r}
x= cats$Bwt
Sxx = sum((x - mean(x)) ^ 2)
se= summary(cat_model)$sigma/sqrt(Sxx)
t_stat= (coef(cat_model)[["Bwt"]]-4)/se
t_stat
```

***
- The p-value of the test

```{r}
2*pt(t_stat, df= length(x)-2, lower.tail = FALSE)
```

***
- A statistical decision at Î±=0.05

At an alpha level of 0.05 we fail to reject the null hypothesis since the p value is greater than the alpha level.

<br>

***

## Exercise 2 (More `lm` for Inference)

For this exercise we will use the `Ozone` dataset from the `mlbench` package. You should use `?Ozone` to learn about the background of this dataset. You may need to install the `mlbench` package. If you do so, do not include code to install the package in your `R` Markdown document.

For simplicity, we will re-perform the data cleaning done in the previous homework.

```{r}
data(Ozone, package = "mlbench")
Ozone = Ozone[, c(4, 6, 7, 8)]
colnames(Ozone) = c("ozone", "wind", "humidity", "temp")
Ozone = Ozone[complete.cases(Ozone), ]
```

**(a)** Fit the following simple linear regression model in `R`. Use the ozone measurement as the response and wind speed as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Store the results in a variable called `ozone_wind_model`. Use a $t$ test to test the significance of the regression. Report the following:

- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.01$
- A conclusion in the context of the problem

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.


<br>
***Solution:***
```{r}
ozone_wind_model=lm(ozone~wind, data=Ozone)
```

- The null and alternative hypotheses

***Null Hypothesis:***
- $H_0: \beta_1 = 0$

There is no significant linear relationship between Ozone levels and the wind speed. 

***Alternate Hypothesis:***
- $H_1: \beta_1 \neq 0$

There is a significant linear relationship between Ozone levels and the wind speed.



***
- The value of the test statistic

The value of test statistic as reported by test for the slope param is:
```{r}
summary(ozone_wind_model)$coefficients[2, "t value"]
```

***

- The p-value of the test

The p-value for the slope param is :
```{r}
summary(ozone_wind_model)$coefficients[2, "Pr(>|t|)"]
```

***
- A statistical decision at $\alpha = 0.01$

At a alpha level of 0.01 we fail to reject the null hypothesis since the p-value reported by test statistic is much greater than the alpha level.

***

- A conclusion in the context of the problem

By conducting the significance of regression test we can infer that there is no relationship between the ozone level and wind speeds and the relationship has a parameter estimate which is not statistically significant from 0.


<br>


**(b)** Fit the following simple linear regression model in `R`. Use the ozone measurement as the response and temperature as the predictor. 

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Store the results in a variable called `ozone_temp_model`. Use a $t$ test to test the significance of the regression. Report the following:

- The null and alternative hypotheses
- The value of the test statistic
- The p-value of the test
- A statistical decision at $\alpha = 0.01$
- A conclusion in the context of the problem

When reporting these, you should explicitly state them in your document, not assume that a reader will find and interpret them from a large block of `R` output.

<br>
***Solution:***
```{r}
ozone_temp_model=lm(ozone~temp, data=Ozone)
```

- The null and alternative hypotheses

***Null Hypothesis:***
- $H_0: \beta_1 = 0$

There is no significant linear relationship between Ozone levels and the temperature. 

***Alternate Hypothesis:***
- $H_1: \beta_1 \neq 0$

There is a significant linear relationship between Ozone levels and the temperature..



***
- The value of the test statistic

The value of test statistic as reported by test for the slope param is:
```{r}
summary(ozone_temp_model)$coefficients[2, "t value"]
```

***

- The p-value of the test

The p-value for the slope param is :
```{r}
summary(ozone_temp_model)$coefficients[2, "Pr(>|t|)"]
```

***
- A statistical decision at $\alpha = 0.01$

At a alpha level of 0.01 we reject the null hypothesis since the p-value reported by test statistic is much lower than the alpha level.

***

- A conclusion in the context of the problem

By conducting the significance of regression test we can infer that there is a relationship between the ozone level and temperature and the relationship has a parameter estimate which is statistically significantly from 0.

<br>

***

## Exercise 3 (Simulating Sampling Distributions)

For this exercise we will simulate data from the following model:

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Where $\epsilon_i \sim N(0, \sigma^2).$ Also, the parameters are known to be:

- $\beta_0 = -5$
- $\beta_1 = 3.25$
- $\sigma^2 = 16$

We will use samples of size $n = 50$.

**(a)** Simulate this model $2000$ times. Each time use `lm()` to fit a simple linear regression model, then store the value of $\hat{\beta}_0$ and $\hat{\beta}_1$. Set a seed using **your** birthday before performing the simulation. Note, we are simulating the $x$ values once, and then they remain fixed for the remainder of the exercise.

```{r}
birthday = 19930501
set.seed(birthday)
n = 50
x = seq(0, 10, length = n)
```


<br>
***Solution:***
```{r}
beta_0  = -5
beta_1  = 3.25
sigma   = 4

Sxx = sum((x - mean(x)) ^ 2)

sim_slr = function(x, beta_0, beta_1, sigma) {
  n = length(x)
  epsilon = rnorm(n, mean = 0, sd = sigma)
  y = beta_0 + beta_1 * x + epsilon
  data.frame(predictor = x, response = y)
}

beta_hat_1 = rep(0, 2000)
beta_hat_0 = rep(0, 2000)
for (i in 1:2000){
    sim_data = sim_slr(x = x, beta_0 = -5, beta_1 = 3.25, sigma = 4)
    sim_fit = lm(response ~ predictor, data = sim_data)
    beta_hat_0[i]=coef(sim_fit)[1]
    beta_hat_1[i]=coef(sim_fit)[2]
}


```
<br>


**(b)** Create a table that summarizes the results of the simulations. The table should have two columns, one for $\hat{\beta}_0$ and one for $\hat{\beta}_1$. The table should have four rows:

- A row for the true expected value given the known values of $x$
- A row for the mean of the simulated values
- A row for the true standard deviation given the known values of $x$
- A row for the standard deviation of the simulated values


<br>
***Solution:***
```{r}
mat <- matrix(, nrow = 4, ncol = 2)
colnames(mat) = c("beta_hat_0", "beta_hat_1")
rownames(mat)=c("Expected Value", "Observed Mean", "True SD", "Observed SD")
mat[1, c(1,2)] <- c(-5, 3.25)
mat[2, c(1,2)] <- c(mean(beta_hat_0), mean(beta_hat_1))
mat[3, c(1,2)] <- c(4*sqrt(((1/50) + (mean(x)^2/Sxx))),4/sqrt(Sxx))
mat[4, c(1,2)] <- c(sd(beta_hat_0), sd(beta_hat_1))
knitr::kable(mat)
```
<br>


**(c)** Plot two histograms side-by-side:

- A histogram of your simulated values for $\hat{\beta}_0$. Add the normal curve for the true sampling distribution of $\hat{\beta}_0$.
- A histogram of your simulated values for $\hat{\beta}_1$. Add the normal curve for the true sampling distribution of $\hat{\beta}_1$.


<br>
***Solution:***
```{r}
par(mfrow=c(1,2))
hist(beta_hat_0, prob = TRUE, breaks = 25, 
     xlab = expression(hat(beta)[0]), main = "Histogram of beta hat 0", border = "black", col="yellow")
curve(dnorm(x, mean = -5, sd = 4*sqrt(((1/50) + (mean(x)^2/Sxx)))),
      col = "darkorange", add = TRUE, lwd = 3)

hist(beta_hat_1, prob = TRUE, breaks = 25, 
     xlab = expression(hat(beta)[1]), main = "Histogram of beta hat 1", border = "black", col="dodgerblue")
curve(dnorm(x, mean = 3.25, sd = 4/sqrt(Sxx)), 
      col = "darkorange", add = TRUE, lwd = 3)


```
<br>


***

## Exercise 4 (Simulating Confidence Intervals)

For this exercise we will simulate data from the following model:

\[
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\]

Where $\epsilon_i \sim N(0, \sigma^2).$ Also, the parameters are known to be:

- $\beta_0 = 5$
- $\beta_1 = 2$
- $\sigma^2 = 9$

We will use samples of size $n = 25$.

Our goal here is to use simulation to verify that the confidence intervals really do have their stated confidence level. Do **not** use the `confint()` function for this entire exercise.

**(a)** Simulate this model $2500$ times. Each time use `lm()` to fit a simple linear regression model, then store the value of $\hat{\beta}_1$ and $s_e$. Set a seed using **your** birthday before performing the simulation. Note, we are simulating the $x$ values once, and then they remain fixed for the remainder of the exercise.

```{r}
birthday = 19930501
set.seed(birthday)
n = 25
x = seq(0, 2.5, length = n)
```

<br>
***Solution:***
```{r}
beta_0  = 5
beta_1  = 2
sigma   = 3

Sxx = sum((x - mean(x)) ^ 2)

sim_slr = function(x, beta_0 = 5, beta_1 = 2, sigma = 3) {
  n = length(x)
  epsilon = rnorm(n, mean = 0, sd = sigma)
  y = beta_0 + beta_1 * x + epsilon
  data.frame(predictor = x, response = y)
}

beta_hat_1 = rep(0, 2500)
s_e_vec= rep(0, 2500)
for (i in 1:2500){
    sim_data = sim_slr(x = x, beta_0 = 5, beta_1 = 2, sigma = 3)
    sim_fit = lm(response ~ predictor, data = sim_data)
    s_e_vec[i]=summary(sim_fit)$sigma
    beta_hat_1[i]=coef(sim_fit)[2]
}


```
<br>

**(b)** For each of the $\hat{\beta}_1$ that you simulated, calculate a 95% confidence interval. Store the lower limits in a vector `lower_95` and the upper limits in a vector `upper_95`. Some hints:

- You will need to use `qt()` to calculate the critical value, which will be the same for each interval.
- Remember that `x` is fixed, so $S_{xx}$ will be the same for each interval.
- You could, but do not need to write a `for` loop. Remember vectorized operations.

<br>
***Solution:***
```{r}
crit_value= qt(0.975, df= n-2)
lower_95 = beta_hat_1-crit_value*(s_e_vec/sqrt(Sxx))
upper_95 = beta_hat_1+crit_value*(s_e_vec/sqrt(Sxx))
```
<br>

**(c)** What proportion of these intervals contains the true value of $\beta_1$?

<br>
***Solution:***
```{r}
mean(2>lower_95 & 2< upper_95)
```
<br>

**(d)** Based on these intervals, what proportion of the simulations would reject the test $H_0: \beta_1 = 0$ vs $H_1: \beta_1 \neq 0$ at $\alpha = 0.05$?


<br>
***Solution:***
```{r}
1-mean(0>lower_95 & 0< upper_95)
```
<br>

**(e)** For each of the $\hat{\beta}_1$ that you simulated, calculate a 99% confidence interval. Store the lower limits in a vector `lower_99` and the upper limits in a vector `upper_99`.

<br>
***Solution:***
```{r}
crit_value= qt(0.995, df= n-2)
lower_99 = beta_hat_1-crit_value*(s_e_vec/sqrt(Sxx))
upper_99 = beta_hat_1+crit_value*(s_e_vec/sqrt(Sxx))
```
<br>

**(f)** What proportion of these intervals contains the true value of $\beta_1$?

<br>
***Solution:***
```{r}
mean(2>lower_99 & 2< upper_99)
```
<br>

**(g)** Based on these intervals, what proportion of the simulations would reject the test $H_0: \beta_1 = 0$ vs $H_1: \beta_1 \neq 0$ at $\alpha = 0.01$?

<br>
***Solution:***
```{r}
1-mean(0>lower_99 & 0 < upper_99)
```
<br>

***

## Exercise 5 (Prediction Intervals "without" `predict`)

Write a function named `calc_pred_int` that performs calculates prediction intervals:

$$
\hat{y}(x) \pm t_{\alpha/2, n - 2} \cdot s_e\sqrt{1 + \frac{1}{n}+\frac{(x-\bar{x})^2}{S_{xx}}}.
$$

for the linear model

$$
Y_i = \beta_0 + \beta_1 x_i + \epsilon_i.
$$

**(a)** Write this function. You may use the `predict()` function, but you may **not** supply a value for the `level` argument of `predict()`. (You can certainly use `predict()` any way you would like in order to check your work.)

The function should take three inputs:

- `model`, a model object that is the result of fitting the SLR model with `lm()`
- `newdata`, a data frame with a single observation (row)
    - This data frame will need to have a variable (column) with the same name as the data used to fit `model`.
- `level`, the level (0.90, 0.95, etc) for the interval with a default value of `0.95`

The function should return a named vector with three elements:

- `estimate`, the midpoint of the interval
- `lower`, the lower bound of the interval
- `upper`, the upper bound of the interval

<br>
***Solution:***
```{r}
calc_pred_int <- function(model, newdata, level=0.95){
  point_est= predict(model, newdata= newdata)
  prop= level+((1-level)/2)
  critic_value = qt(prop, df= length(model$residuals)-2)
  s_e= summary(model)$sigma
  x_vals= model$model[,2]
  Sxx = sum((x_vals - mean(x_vals)) ^ 2)
  
  temp1= sqrt(1+ (1/length(resid(model)))  + ((newdata$Bwt-mean(x_vals))^2/ Sxx))
  
  lower= point_est - critic_value * s_e*temp1
  upper= point_est + critic_value * s_e*temp1
  
  return (list(estimate= point_est, lower=lower, upper=upper))
  
}
```
<br>

**(b)** After writing the function, run this code:

```{r, eval = TRUE}
newcat_1 = data.frame(Bwt = 4.0)
calc_pred_int(cat_model, newcat_1)
```


**(c)** After writing the function, run this code:

```{r, eval = TRUE}
newcat_2 = data.frame(Bwt = 3.3)
calc_pred_int(cat_model, newcat_2, level = 0.99)
```


***